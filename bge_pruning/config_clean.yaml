# Production BGE-M3 Pruning Configuration
# Focused on head/layer/intermediate pruning with MTEB support

model:
  base_model: "BAAI/bge-m3"
  use_sts_loss: true
  use_contrastive_loss: true
  temperature: 0.02
  
  l0_module:
    # Production pruning: head, layer, intermediate only
    pruning_modules: ["layer", "head", "intermediate"]
    start_sparsity: 0.0
    lagrangian_warmup_steps: "1000ba"
    eval_target_model: true
    
    # Target architecture for 75% reduction (production setting)
    target_model:
      n_layers: 18  # From 24
      n_heads: 12   # From 16
      intermediate_size: 3072  # From 4096

optimizer:
  # Stable learning rates for production
  lr: 2.0e-5
  betas: [0.9, 0.999]
  eps: 1.0e-8
  weight_decay: 0.01

# Training settings
batch_size: 16
max_length: 512
max_duration: "1000ba"
eval_interval: "100ba"
save_interval: "500ba"
save_folder: "experiments/production"
seed: 42
device: gpu
precision: "amp_bf16"

# HuggingFace model export (automatic after training)
# Saves to: {save_folder}_hf/