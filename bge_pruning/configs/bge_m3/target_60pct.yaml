model:
  name: composer_bge_m3
  base_model: "BAAI/bge-m3"
  d_model: 1024
  n_heads: 16
  n_layers: 24
  intermediate_size: 4096
  vocab_size: 250002
  max_position_embeddings: 8192
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  layer_norm_eps: 1e-12
  use_sts_loss: true
  use_contrastive_loss: true
  temperature: 0.02

  l0_module:
    pruning_modules: ["layer", "head", "intermediate", "hidden"]
    start_sparsity: 0.0
    lagrangian_warmup_steps: "1000ba"
    eval_target_model: true
    target_model:
      d_model: 614        # 60% of 1024
      n_heads: 10         # 60% of 16
      n_layers: 14        # 60% of 24
      intermediate_size: 2458  # 60% of 4096
      vocab_size: 250002

tokenizer:
  name: "BAAI/bge-m3"
  max_length: 512

optimizer:
  name: adamw
  lr: 1.0e-5  # Lower for aggressive pruning
  betas: [0.9, 0.999]
  eps: 1.0e-8
  weight_decay: 0.01

scheduler:
  name: linear_decay_with_warmup
  t_warmup: "1000ba"
  alpha_f: 0.0

train_loader:
  name: mixed
  dataset:
    sts_path: "data/sts/train.tsv"
    mteb_path: "data/mteb/train.jsonl"
    sts_ratio: 0.5
  batch_size: 32
  num_workers: 4
  persistent_workers: true
  pin_memory: true
  timeout: 0

eval_loader:
  name: sts
  dataset:
    sts_path: "data/sts/dev.tsv"
  batch_size: 64
  num_workers: 2
  persistent_workers: false
  pin_memory: true
  timeout: 0

max_duration: "15000ba"
eval_interval: "500ba"
save_interval: "1000ba"
save_folder: "experiments/checkpoints/target_60pct"

loggers:
  wandb:
    project: "bge-m3-pruning"
    name: "target-60pct"

callbacks:
  - name: "embedding_callback"
  - name: "evaluation_callback"
  - name: "pruning_callback"

precision: "amp_bf16"
device_train_microbatch_size: "auto"

run_name: "bge_m3_target_60pct"
